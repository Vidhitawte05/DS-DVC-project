# -*- coding: utf-8 -*-
"""DS_EXP_8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b7FZybOMnJiRTDNLgkY9rGCfIcvcsCQq

# Task
Create a Streamlit/Dash dashboard, write a Responsible AI report, and publish the final project repository on GitHub, including the dashboard application, models, notebooks, and the Responsible AI report.

## Load and preprocess data

### Subtask:
Load the data required for the dashboard and models. Perform any necessary preprocessing steps.

**Reasoning**:
Load the data into a pandas DataFrame and display the first 5 rows and the data types.
"""

import pandas as pd

df = pd.read_excel('/content/cleaned_samsung_data-1.xlsx')
display(df.head())
display(df.info())

"""**Reasoning**:
Check for missing values in each column and display the count of missing values per column.


"""

display(df.isnull().sum())

"""**Reasoning**:
Since there are no missing values, the next step is to identify categorical columns and apply one-hot encoding, then scale the numerical features, and finally display the head and info of the preprocessed dataframe.


"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

categorical_features = ['name', 'storage_ram', 'os_processor', 'camera', 'display', 'network', 'battery']
numerical_features = ['ratings', 'ram_gb']  # remove 'price'



preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)])

preprocessed_df = preprocessor.fit_transform(df)
preprocessed_df = pd.DataFrame(preprocessed_df)

print(preprocessed_df.head())
print(preprocessed_df.info())

"""## Build and evaluate models

### Subtask:
Build and evaluate the machine learning models that will be used in the dashboard for predictions and insights.

**Reasoning**:
Split the data into training and testing sets and train a Linear Regression model. Evaluate the model using MAE, MSE, and R-squared.
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import joblib
X = df[categorical_features + numerical_features]
y = df['price']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

X_processed = preprocessor.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"MAE: {mae}")
print(f"MSE: {mse}")
print(f"RMSE: {rmse}")
print(f"RÂ²: {r2}")

joblib.dump(model, 'linear_regression_model.pkl')
joblib.dump(preprocessor, 'preprocessor.pkl')
joblib.dump({'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}, 'evaluation_metrics.pkl')

# Add this to your training notebook
import numpy as np
import joblib

# Take a random sample of 100 data points from your training set
# and convert it from a sparse matrix to a regular (dense) array.
if X_train.shape[0] > 100:
    sample_indices = np.random.choice(X_train.shape[0], 100, replace=False)
    X_train_sample = X_train[sample_indices].toarray()
else:
    X_train_sample = X_train.toarray()

# Save this sample to a new file
joblib.dump(X_train_sample, 'X_train_sample.pkl')

print("Training data sample saved successfully to 'X_train_sample.pkl'")

"""**Reasoning**:
The error indicates an issue with the sparse matrix format when used with `LinearRegression`. Convert the sparse matrix to a dense array before splitting the data and training the model.

## Develop dashboard

### Subtask:
Create the Streamlit or Dash application to display predictions, insights, and visualizations. Include sections for metrics, drift checks, and visualizations like SHAP plots.

**Reasoning**:
Create the Streamlit app file and include the necessary imports, load the model and metrics, and define the basic layout with sections for metrics and prediction input.

**Reasoning**:
The previous command failed because the `streamlit` library is not installed. Install `streamlit` and other necessary libraries using `pip`.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit joblib pandas numpy matplotlib seaborn plotly

"""**Reasoning**:
Now that the necessary libraries are installed, create the Streamlit app file again with the imports, load the model and metrics, and set up the basic layout with sections for metrics and prediction input.

**Reasoning**:
The Streamlit app file has been created with the basic structure and input fields. However, the preprocessing step for user input is a placeholder and needs to be implemented correctly. Since the original preprocessor object was not saved, I will add code to save the preprocessor object and then update the Streamlit app code to load and use it for transforming user input before making a prediction.
"""

# Save the preprocessor object
joblib.dump(preprocessor, 'preprocessor.pkl')

"""**Reasoning**:
The preprocessor object has been saved. Now, update the Streamlit app code to load the preprocessor and use it to transform the user input before making a prediction. Also, refine the input fields to better match the expected data types and potentially provide dropdowns for categorical features if the unique values are accessible.

## Write responsible ai report

### Subtask:
Document the responsible AI considerations, including fairness, privacy, and consent, in a markdown file (`Responsible_AI.md`).

**Reasoning**:
Create the `Responsible_AI.md` file and add the content as per the instructions, including sections on fairness, privacy, consent, and transparency/explainability.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile Responsible_AI.md
# # Responsible AI Report for Samsung Phone Price Prediction Dashboard
# 
# This report outlines the responsible AI considerations for the Samsung Phone Price Prediction Dashboard project, focusing on fairness, privacy, consent, and transparency.
# 
# ## Fairness
# 
# Potential biases in the data or model could arise if the training data does not adequately represent the diverse range of Samsung phone characteristics or if certain feature combinations are underrepresented. This could lead to less accurate predictions for specific phone models or price ranges. For example, if the dataset has very few high-end or low-end phones, the model might perform poorly on those extremes.
# 
# To address potential biases:
# - **Data Collection and Representation:** Ensure the training data is as comprehensive and representative as possible across different phone series, price points, and feature sets.
# - **Monitoring Model Performance:** Continuously monitor the model's performance on different subsets of data (e.g., grouped by price range, release year if available) to identify and address disparities in prediction accuracy.
# - **Bias Detection Tools:** In a more advanced scenario, tools for detecting bias in machine learning models could be employed.
# 
# While the current Linear Regression model is relatively simple, future, more complex models would require more rigorous fairness testing.
# 
# ## Privacy
# 
# This application is designed with user privacy in mind. The dashboard currently uses direct user input solely for the purpose of generating a price prediction for that specific input.
# 
# - **No Personal Data Collection:** The application does not collect, store, or process any personally identifiable information from users.
# - **Ephemeral Data Handling:** User input for the prediction is used in real-time and is not stored after the prediction is made.
# 
# If the application were to evolve to include features that require data collection or storage (e.g., saving user preferences, tracking usage), the following privacy measures would be implemented:
# - **Anonymization/Pseudonymization:** Any collected data would be anonymized or pseudonymized to protect user identities.
# - **Secure Storage:** Data would be stored securely using encryption and access controls.
# - **Clear Data Retention Policies:** Data would only be retained for as long as necessary for the intended purpose.
# 
# ## Consent
# 
# Given the current functionality of the dashboard, which relies on voluntary user input for a single prediction and does not collect or store data, explicit consent mechanisms beyond the user's decision to use the prediction feature are not required.
# 
# - **Voluntary Input:** Users choose to provide input to get a prediction. Their action of entering data and clicking "Predict" implies their consent for that specific prediction request.
# 
# Should the application's scope expand to include features that involve collecting, storing, or processing user data, informed consent would be obtained through:
# - **Clear Notifications:** Users would be clearly informed about what data is being collected, why it is being collected, and how it will be used.
# - **Opt-in Mechanisms:** Explicit opt-in mechanisms (e.g., checkboxes) would be provided for users to agree to data collection and processing.
# - **Easy Withdrawal of Consent:** Users would have the ability to easily withdraw their consent at any time.
# 
# ## Transparency and Explainability
# 
# Transparency in this project relates to understanding how the model arrives at its predictions. The current model is a Linear Regression model, which is generally considered interpretable. The coefficients of the linear model can provide some insight into the relationship between features and the predicted price.
# 
# For future enhancements, as indicated in the dashboard, SHAP (SHapley Additive exPlanations) plots are planned. SHAP values provide a way to explain the output of any machine learning model. They indicate how much each feature contributes to the prediction for a specific instance, offering local interpretability. This would allow users or developers to understand why a particular phone received a certain price prediction.
# 
# ## Other Considerations
# 
# - **Model Monitoring:** Beyond data drift, continuous monitoring of model performance in a production environment is crucial to detect performance degradation over time due to concept drift or changes in the underlying data distribution.
# - **Security:** Implementing appropriate security measures to protect the dashboard and the underlying infrastructure from cyber threats is essential.
# - **Accessibility:** Ensuring the dashboard is accessible to users with disabilities would be an important consideration for broader deployment.

"""## Prepare and publish repository

### Subtask:
Organize all the code, notebooks, reports, and the dashboard application files. Publish everything to a public GitHub repository.

## Prepare and publish repository

### Subtask:
Organize all the code, notebooks, reports, and the dashboard application files. Publish everything to a public GitHub repository.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import joblib
# import pandas as pd
# import shap
# import matplotlib.pyplot as plt
# 
# # --------------------------
# # Load models and data
# # --------------------------
# try:
#     model = joblib.load('linear_regression_model.pkl')
#     preprocessor = joblib.load('preprocessor.pkl')
#     # Load the new training data sample
#     X_train_sample = joblib.load('X_train_sample.pkl')
# except FileNotFoundError as e:
#     st.error(f"Could not load a required file. Please ensure all .pkl files are present. Error: {e}")
#     st.stop()
# 
# # --------------------------
# # Streamlit UI
# # --------------------------
# st.title('ð± Samsung Phone Price Prediction Dashboard')
# st.write('Predict Samsung phone prices based on their specifications.')
# 
# # User input section
# st.header('ð§¾ Enter Phone Specifications')
# col1, col2 = st.columns(2)
# with col1:
#     ratings = st.number_input('Ratings', min_value=1.0, max_value=5.0, value=4.3, step=0.1)
#     ram_gb = st.number_input('RAM (GB)', min_value=1, max_value=16, value=6)
#     name = st.text_input('Phone Name', 'SAMSUNG Galaxy F23 5G')
#     storage_ram = st.text_input('Storage/RAM Details', 'Internal Storage128 GBRAM6 GB')
# 
# with col2:
#     os_processor = st.text_input('OS/Processor Details', 'Operating SystemAndroid 12Processor TypeQualcomm')
#     camera = st.text_input('Camera Details', 'Primary Camera50MP + 8MP + 2MP')
#     display = st.text_input('Display Details', 'Display Size16.76 cm (6.6 inch)')
#     network = st.text_input('Network Details', 'Network Type5G, 4G, 3G, 2G')
#     battery = st.text_input('Battery Details', 'Battery Capacity5000 mAh')
# 
# # Prediction logic
# if st.button('Predict Price'):
#     try:
#         input_data = pd.DataFrame({
#             'name': [name], 'storage_ram': [storage_ram], 'os_processor': [os_processor],
#             'camera': [camera], 'display': [display], 'network': [network], 'battery': [battery],
#             'ratings': [ratings], 'ram_gb': [ram_gb]
#         })
# 
#         processed_input = preprocessor.transform(input_data)
#         predicted_price = model.predict(processed_input)[0]
# 
#         st.success(f"ð° **Predicted Price: â¹{predicted_price:,.2f}**")
#         st.subheader('ð Feature Impact on this Prediction')
# 
#         explainer = shap.LinearExplainer(model, X_train_sample)
#         shap_values = explainer.shap_values(processed_input)
# 
#         feature_names = preprocessor.get_feature_names_out()
#         cleaned_names = [name.split('__')[-1] for name in feature_names]
# 
#         explanation = shap.Explanation(
#             values=shap_values[0],
#             base_values=explainer.expected_value,
#             data=processed_input.toarray()[0],
#             feature_names=cleaned_names
#         )
# 
#         # --- PLOT SIZE FIX ---
#         # 1. Create a figure with a specific size (width, height in inches)
#         fig, ax = plt.subplots(figsize=(10, 10))
# 
#         # 2. Create the waterfall plot
#         shap.waterfall_plot(explanation, max_display=15, show=False)
# 
#         # 3. Use tight_layout to ensure everything fits without overlapping
#         plt.tight_layout()
#         st.pyplot(fig)
# 
# 
#     except Exception as e:
#         st.error(f"An error occurred: {e}")
# 
# st.caption("Developed by Pratik â Samsung Phone Price ML Project")

"""## Finalize deliverables

### Subtask:
Ensure the Streamlit app is deployed and accessible via a link. Verify that the `Responsible_AI.md` file is complete and included in the repository. Confirm the final public repository link is ready to be shared.

## Summary:

### Data Analysis Key Findings

*   The dataset contained both numerical and categorical features with no missing values.
*   Preprocessing involved scaling numerical features using `StandardScaler` and encoding categorical features using `OneHotEncoder`, resulting in a sparse matrix format.
*   A `LinearRegression` model was trained to predict phone prices, achieving very low error metrics (MAE, MSE, RMSE) and an R-squared of 1.0 on the test set, indicating a perfect fit which might suggest overfitting or data characteristics leading to a simple linear relationship.
*   The trained model and the preprocessing object were successfully saved for later use in the dashboard.
*   A Streamlit application was developed to serve as the dashboard, capable of loading the saved model and preprocessor, displaying evaluation metrics, accepting user input for phone features, and providing price predictions.
*   A `Responsible_AI.md` file was created, outlining considerations for fairness, privacy, consent, and transparency related to the project.

### Insights or Next Steps

*   Investigate the unusually high R-squared value (1.0) to understand if it indicates overfitting or a dataset where a simple linear model perfectly captures the price relationship, and consider more robust evaluation or cross-validation if necessary.
*   Complete the planned dashboard features, including implementing data drift checks and generating SHAP plots for model interpretability to enhance the dashboard's functionality and adherence to responsible AI principles.
"""

!ls

!ngrok config add-authtoken 339SyYXu0MWRMf1BQQblF6sLw3A_52kP9WzMvTthtvM8Gzz7Z

!pip install pyngrok

from pyngrok import ngrok
import subprocess

# Run Streamlit app
print("Starting Streamlit server...")
public_url = ngrok.connect(8501)
print(f"Public URL: {public_url}")
subprocess.run(["streamlit", "run", "app.py", "--server.port", "8501"])